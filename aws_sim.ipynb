{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d0efd4-f037-4ce2-8a7d-f2604e6272d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/anaconda3/etc/profile.d/conda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d59729-d89f-432e-b334-fc75188a46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/torch_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    zstd-1.5.7                 |       h3691f8a_5         537 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         537 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_14 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_14 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_14 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-he9a06e4_0 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.3-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.10.19-h3c07f61_2_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-h3691f8a_5 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Collecting torch==2.3.0\n",
      "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchtext==0.18.0\n",
      "  Using cached torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting torchquantum\n",
      "  Using cached torchquantum-0.1.8-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting filelock (from torch==2.3.0)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.3.0)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy (from torch==2.3.0)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.3.0)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.3.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.3.0)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0)\n",
      "  Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting tqdm (from torchtext==0.18.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests (from torchtext==0.18.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting numpy (from torchtext==0.18.0)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting dill==0.3.4 (from torchquantum)\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting matplotlib>=3.3.2 (from torchquantum)\n",
      "  Using cached matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting nbsphinx (from torchquantum)\n",
      "  Using cached nbsphinx-0.9.8-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opt-einsum (from torchquantum)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pathos>=0.2.7 (from torchquantum)\n",
      "  Using cached pathos-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pylatexenc>=2.10 (from torchquantum)\n",
      "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
      "Collecting pyscf>=2.0.1 (from torchquantum)\n",
      "  Using cached pyscf-2.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting qiskit<1.0.0,>=0.39.0 (from torchquantum)\n",
      "  Using cached qiskit-0.46.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting recommonmark (from torchquantum)\n",
      "  Using cached recommonmark-0.7.1-py2.py3-none-any.whl.metadata (463 bytes)\n",
      "Collecting scipy>=1.5.2 (from torchquantum)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: setuptools>=52.0.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torchquantum) (80.9.0)\n",
      "Collecting tensorflow>=2.4.1 (from torchquantum)\n",
      "  Using cached tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting torchdiffeq>=0.2.3 (from torchquantum)\n",
      "  Using cached torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting torchpack>=0.3.0 (from torchquantum)\n",
      "  Using cached torchpack-0.3.1-py3-none-any.whl.metadata (533 bytes)\n",
      "Collecting torchvision>=0.9.0.dev20210130 (from torchquantum)\n",
      "  Using cached torchvision-0.24.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting qiskit-terra==0.46.3 (from qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached qiskit_terra-0.46.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting rustworkx>=0.13.0 (from qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting ply>=3.10 (from qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Collecting psutil>=5 (from qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting python-dateutil>=2.8.0 (from qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting stevedore>=3.0.0 (from qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached stevedore-5.6.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting symengine>=0.11 (from qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached symengine-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (113 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.3.2->torchquantum)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting ppft>=1.7.7 (from pathos>=0.2.7->torchquantum)\n",
      "  Using cached ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
      "INFO: pip is looking at multiple versions of pathos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pathos>=0.2.7 (from torchquantum)\n",
      "  Using cached pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached pathos-0.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached pathos-0.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached pathos-0.2.9-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached pathos-0.2.8-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting multiprocess>=0.70.12 (from pathos>=0.2.7->torchquantum)\n",
      "  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pox>=0.3.0 (from pathos>=0.2.7->torchquantum)\n",
      "  Using cached pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess>=0.70.12 (from pathos>=0.2.7->torchquantum)\n",
      "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
      "  Using cached multiprocess-0.70.13-py310-none-any.whl.metadata (6.8 kB)\n",
      "  Using cached multiprocess-0.70.12.2-py39-none-any.whl.metadata (6.9 kB)\n",
      "Collecting h5py>=2.7 (from pyscf>=2.0.1->torchquantum)\n",
      "  Using cached h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.0->qiskit-terra==0.46.3->qiskit<1.0.0,>=0.39.0->torchquantum)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached ml_dtypes-0.5.4-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->torchtext==0.18.0)\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchtext==0.18.0)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchtext==0.18.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchtext==0.18.0)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.4.1->torchquantum) (0.45.1)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached optree-0.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Collecting loguru (from torchpack>=0.3.0->torchquantum)\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting multimethod (from torchpack>=0.3.0->torchquantum)\n",
      "  Using cached multimethod-2.0.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyyaml (from torchpack>=0.3.0->torchquantum)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting tensorpack (from torchpack>=0.3.0->torchquantum)\n",
      "  Using cached tensorpack-0.11-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting toml (from torchpack>=0.3.0->torchquantum)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision>=0.9.0.dev20210130 (from torchquantum)\n",
      "  Using cached torchvision-0.24.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "  Using cached torchvision-0.23.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "  Using cached torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "  Using cached torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Using cached torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting markupsafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting docutils>=0.18.1 (from nbsphinx->torchquantum)\n",
      "  Using cached docutils-0.22.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting nbconvert!=5.4,>=5.3 (from nbsphinx->torchquantum)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nbformat (from nbsphinx->torchquantum)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting sphinx!=8.2.0,!=8.2.1,>=1.8 (from nbsphinx->torchquantum)\n",
      "  Using cached sphinx-8.1.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting traitlets>=5 (from nbsphinx->torchquantum)\n",
      "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached bleach-6.3.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting defusedxml (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyter-core>=4.7 (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached jupyter_core-5.9.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pygments>=2.4.1 (from nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core>=4.7->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached platformdirs-4.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jupyter-client>=6.1.12 (from nbclient>=0.5.0->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyzmq>=23.0 (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached pyzmq-27.1.0-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting tornado>=6.2 (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat->nbsphinx->torchquantum)\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat->nbsphinx->torchquantum)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat->nbsphinx->torchquantum)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->nbsphinx->torchquantum)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->nbsphinx->torchquantum)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat->nbsphinx->torchquantum)\n",
      "  Using cached rpds_py-0.30.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting docutils>=0.18.1 (from nbsphinx->torchquantum)\n",
      "  Using cached docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting babel>=2.13 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting alabaster>=0.7.14 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tomli>=2 (from sphinx!=8.2.0,!=8.2.1,>=1.8->nbsphinx->torchquantum)\n",
      "  Using cached tomli-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4->nbconvert!=5.4,>=5.3->nbsphinx->torchquantum)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting commonmark>=0.8.1 (from recommonmark->torchquantum)\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.4.1->torchquantum)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tabulate>=0.7.7 (from tensorpack->torchpack>=0.3.0->torchquantum)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting msgpack>=0.5.2 (from tensorpack->torchpack>=0.3.0->torchquantum)\n",
      "  Using cached msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting msgpack-numpy>=0.4.4.2 (from tensorpack->torchpack>=0.3.0->torchquantum)\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "Using cached torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Using cached torchquantum-0.1.8-py3-none-any.whl (267 kB)\n",
      "Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Using cached qiskit-0.46.3-py3-none-any.whl (10.0 kB)\n",
      "Using cached qiskit_terra-0.46.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
      "Using cached multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "Using cached pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Using cached pox-0.3.6-py3-none-any.whl (29 kB)\n",
      "Using cached ppft-1.7.7-py3-none-any.whl (56 kB)\n",
      "Using cached psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pyscf-2.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51.3 MB)\n",
      "Using cached h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached stevedore-5.6.0-py3-none-any.whl (54 kB)\n",
      "Using cached symengine-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.4 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "Using cached grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "Using cached ml_dtypes-0.5.4-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
      "Using cached torchpack-0.3.1-py3-none-any.whl (34 kB)\n",
      "Using cached torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Using cached wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (113 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached multimethod-2.0.2-py3-none-any.whl (9.6 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached nbsphinx-0.9.8-py3-none-any.whl (31 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached jupyter_core-5.9.1-py3-none-any.whl (29 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached platformdirs-4.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached pyzmq-27.1.0-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (854 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.30.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n",
      "Using cached sphinx-8.1.3-py3-none-any.whl (3.5 MB)\n",
      "Using cached docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "Using cached alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
      "Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Using cached tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "Using cached tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "Using cached optree-0.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (386 kB)\n",
      "Using cached pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
      "Using cached recommonmark-0.7.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
      "Using cached msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
      "Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: webencodings, pylatexenc, ply, namex, mpmath, libclang, flatbuffers, fastjsonschema, commonmark, wrapt, urllib3, typing-extensions, traitlets, tqdm, tornado, tomli, toml, tinycss2, termcolor, tensorboard-data-server, tabulate, sympy, symengine, stevedore, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, soupsieve, snowballstemmer, six, rpds-py, pyzmq, pyyaml, pyparsing, pygments, psutil, protobuf, ppft, pox, platformdirs, pillow, pandocfilters, packaging, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multimethod, msgpack, mdurl, markupsafe, markdown, loguru, kiwisolver, jupyterlab-pygments, imagesize, idna, gast, fsspec, fonttools, filelock, docutils, dill, defusedxml, cycler, charset_normalizer, certifi, bleach, babel, attrs, alabaster, absl-py, werkzeug, triton, scipy, rustworkx, requests, referencing, python-dateutil, optree, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, msgpack-numpy, ml_dtypes, mistune, markdown-it-py, jupyter-core, jinja2, h5py, grpcio, google_pasta, contourpy, beautifulsoup4, astunparse, tensorpack, tensorboard, sphinx, rich, qiskit-terra, pyscf, pathos, nvidia-cusolver-cu12, matplotlib, jupyter-client, jsonschema-specifications, torch, recommonmark, qiskit, keras, jsonschema, torchvision, torchtext, torchdiffeq, tensorflow, nbformat, torchpack, nbclient, nbconvert, nbsphinx, torchquantum\n",
      "\u001b[?25l\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m  4/132\u001b[0m [mpmath]\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m  4/132\u001b[0m [mpmath]\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m  5/132\u001b[0m [libclang]\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m  5/132\u001b[0m [libclang]\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m  5/132\u001b[0m [libclang]\n",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m  9/132\u001b[0m [wrapt]\n",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 12/132\u001b[0m [traitlets]\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 14/132\u001b[0m [tornado]\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 15/132\u001b[0m [tomli]\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 19/132\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 21/132\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/132\u001b[0m [symengine]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/132\u001b[0m [symengine]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/132\u001b[0m [symengine]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/132\u001b[0m [symengine]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 22/132\u001b[0m [symengine]\n",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 25/132\u001b[0m [sphinxcontrib-qthelp]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 31/132\u001b[0m [snowballstemmer]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 32/132\u001b[0m [six]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 35/132\u001b[0m [pyyaml]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/132\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/132\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/132\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/132\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/132\u001b[0m [pygments]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 39/132\u001b[0m [protobuf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 41/132\u001b[0m [pox]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 43/132\u001b[0m [pillow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 43/132\u001b[0m [pillow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 46/132\u001b[0m [opt-einsum]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 48/132\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 48/132\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 48/132\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 48/132\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/132\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 50/132\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 50/132\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 50/132\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 50/132\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 51/132\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 53/132\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 53/132\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 53/132\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 53/132\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 54/132\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 54/132\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 55/132\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 56/132\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 57/132\u001b[0m [networkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 62/132\u001b[0m [markdown]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 64/132\u001b[0m [kiwisolver]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 68/132\u001b[0m [gast]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 70/132\u001b[0m [fonttools]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 72/132\u001b[0m [docutils]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 72/132\u001b[0m [docutils]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 72/132\u001b[0m [docutils]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 74/132\u001b[0m [defusedxml]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 78/132\u001b[0m [bleach]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 79/132\u001b[0m [babel]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 79/132\u001b[0m [babel]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 79/132\u001b[0m [babel]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 82/132\u001b[0m [absl-py]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 83/132\u001b[0m [werkzeug]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 84/132\u001b[0m [triton]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 85/132\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m 86/132\u001b[0m [rustworkx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m 89/132\u001b[0m [python-dateutil]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 91/132\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 92/132\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 93/132\u001b[0m [multiprocess]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 95/132\u001b[0m [ml_dtypes]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m 96/132\u001b[0m [mistune]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m 99/132\u001b[0m [jinja2]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m100/132\u001b[0m [h5py]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m100/132\u001b[0m [h5py]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m101/132\u001b[0m [grpcio]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m101/132\u001b[0m [grpcio]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m103/132\u001b[0m [contourpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m106/132\u001b[0m [tensorpack]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m106/132\u001b[0m [tensorpack]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m107/132\u001b[0m [tensorboard]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m107/132\u001b[0m [tensorboard]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m108/132\u001b[0m [sphinx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m108/132\u001b[0m [sphinx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m108/132\u001b[0m [sphinx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m108/132\u001b[0m [sphinx]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m109/132\u001b[0m [rich]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m110/132\u001b[0m [qiskit-terra]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m111/132\u001b[0m [pyscf]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m113/132\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m114/132\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━���━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m117/132\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m120/132\u001b[0m [keras]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m120/132\u001b[0m [keras]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m120/132\u001b[0m [keras]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m120/132\u001b[0m [keras]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m120/132\u001b[0m [keras]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m120/132\u001b[0m [keras]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m121/132\u001b[0m [jsonschema]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m122/132\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m122/132\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m122/132\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m123/132\u001b[0m [torchtext]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m125/132\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m126/132\u001b[0m [nbformat]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m129/132\u001b[0m [nbconvert]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m131/132\u001b[0m [torchquantum]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132/132\u001b[0m [torchquantum]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 alabaster-1.0.0 astunparse-1.6.3 attrs-25.4.0 babel-2.17.0 beautifulsoup4-4.14.3 bleach-6.3.0 certifi-2025.11.12 charset_normalizer-3.4.4 commonmark-0.9.1 contourpy-1.3.2 cycler-0.12.1 defusedxml-0.7.1 dill-0.3.4 docutils-0.21.2 fastjsonschema-2.21.2 filelock-3.20.0 flatbuffers-25.9.23 fonttools-4.61.0 fsspec-2025.10.0 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 idna-3.11 imagesize-1.4.1 jinja2-3.1.6 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-client-8.6.3 jupyter-core-5.9.1 jupyterlab-pygments-0.3.0 keras-3.12.0 kiwisolver-1.4.9 libclang-18.1.1 loguru-0.7.3 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 matplotlib-3.10.7 mdurl-0.1.2 mistune-3.1.4 ml_dtypes-0.5.4 mpmath-1.3.0 msgpack-1.1.2 msgpack-numpy-0.4.8 multimethod-2.0.2 multiprocess-0.70.12.2 namex-0.1.0 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nbsphinx-0.9.8 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 opt-einsum-3.4.0 optree-0.18.0 packaging-25.0 pandocfilters-1.5.1 pathos-0.2.8 pillow-12.0.0 platformdirs-4.5.0 ply-3.11 pox-0.3.6 ppft-1.7.7 protobuf-6.33.1 psutil-7.1.3 pygments-2.19.2 pylatexenc-2.10 pyparsing-3.2.5 pyscf-2.11.0 python-dateutil-2.9.0.post0 pyyaml-6.0.3 pyzmq-27.1.0 qiskit-0.46.3 qiskit-terra-0.46.3 recommonmark-0.7.1 referencing-0.37.0 requests-2.32.5 rich-14.2.0 rpds-py-0.30.0 rustworkx-0.17.1 scipy-1.15.3 six-1.17.0 snowballstemmer-3.0.1 soupsieve-2.8 sphinx-8.1.3 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 stevedore-5.6.0 symengine-0.14.1 sympy-1.14.0 tabulate-0.9.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 tensorpack-0.11 termcolor-3.2.0 tinycss2-1.4.0 toml-0.10.2 tomli-2.3.0 torch-2.3.0 torchdiffeq-0.2.5 torchpack-0.3.1 torchquantum-0.1.8 torchtext-0.18.0 torchvision-0.18.0 tornado-6.5.2 tqdm-4.67.1 traitlets-5.14.3 triton-2.3.0 typing-extensions-4.15.0 urllib3-2.5.0 webencodings-0.5.1 werkzeug-3.1.4 wrapt-2.0.1\n",
      "\n",
      "Collecting amazon-braket-sdk\n",
      "  Using cached amazon_braket_sdk-1.106.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting amazon-braket-pennylane-plugin\n",
      "  Using cached amazon_braket_pennylane_plugin-1.33.6-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting amazon-braket-schemas>=1.26.1 (from amazon-braket-sdk)\n",
      "  Using cached amazon_braket_schemas-1.27.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting amazon-braket-default-simulator>=1.32.0 (from amazon-braket-sdk)\n",
      "  Using cached amazon_braket_default_simulator-1.32.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting oqpy~=0.3.7 (from amazon-braket-sdk)\n",
      "  Using cached oqpy-0.3.9-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting backoff (from amazon-braket-sdk)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting boltons (from amazon-braket-sdk)\n",
      "  Using cached boltons-25.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting boto3>=1.28.53 (from amazon-braket-sdk)\n",
      "  Using cached boto3-1.42.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting cloudpickle==2.2.1 (from amazon-braket-sdk)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting nest-asyncio (from amazon-braket-sdk)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from amazon-braket-sdk) (3.4.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from amazon-braket-sdk) (2.2.6)\n",
      "Collecting openpulse (from amazon-braket-sdk)\n",
      "  Using cached openpulse-1.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting openqasm3 (from amazon-braket-sdk)\n",
      "  Using cached openqasm3-1.0.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from amazon-braket-sdk) (1.14.0)\n",
      "Collecting backports.entry-points-selectable (from amazon-braket-sdk)\n",
      "  Using cached backports.entry_points_selectable-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.2.0 (from oqpy~=0.3.7->amazon-braket-sdk)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting autoray>=0.6.11 (from amazon-braket-pennylane-plugin)\n",
      "  Using cached autoray-0.8.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting pennylane>=0.42.0 (from amazon-braket-pennylane-plugin)\n",
      "  Using cached pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting numba (from amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: opt_einsum in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk) (3.4.0)\n",
      "Collecting pydantic>2 (from amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk) (1.15.3)\n",
      "Collecting antlr4-python3-runtime==4.13.2 (from amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
      "Collecting botocore<1.42.0,>=1.41.6 (from boto3>=1.28.53->amazon-braket-sdk)\n",
      "  Using cached botocore-1.41.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.53->amazon-braket-sdk)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3>=1.28.53->amazon-braket-sdk)\n",
      "  Using cached s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from botocore<1.42.0,>=1.41.6->boto3>=1.28.53->amazon-braket-sdk) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from botocore<1.42.0,>=1.41.6->boto3>=1.28.53->amazon-braket-sdk) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.42.0,>=1.41.6->boto3>=1.28.53->amazon-braket-sdk) (1.17.0)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pennylane>=0.42.0->amazon-braket-pennylane-plugin) (0.17.1)\n",
      "Collecting autograd (from pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting appdirs (from pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting autoray>=0.6.11 (from amazon-braket-pennylane-plugin)\n",
      "  Using cached autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools (from pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pennylane-lightning>=0.42 (from pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached pennylane_lightning-0.42.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pennylane>=0.42.0->amazon-braket-pennylane-plugin) (2.32.5)\n",
      "Collecting tomlkit (from pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pennylane>=0.42.0->amazon-braket-pennylane-plugin) (4.15.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pennylane>=0.42.0->amazon-braket-pennylane-plugin) (25.0)\n",
      "Collecting diastatic-malt (from pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane>=0.42.0->amazon-braket-pennylane-plugin)\n",
      "  Using cached scipy_openblas32-0.3.30.0.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>2->amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>2->amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached pydantic_core-2.41.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>2->amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: astunparse in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from diastatic-malt->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (1.6.3)\n",
      "Requirement already satisfied: gast in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from diastatic-malt->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (0.7.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from diastatic-malt->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (3.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from astunparse->diastatic-malt->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (0.45.1)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba->amazon-braket-default-simulator>=1.32.0->amazon-braket-sdk)\n",
      "  Using cached llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests->pennylane>=0.42.0->amazon-braket-pennylane-plugin) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from sympy->amazon-braket-sdk) (1.3.0)\n",
      "Using cached amazon_braket_sdk-1.106.1-py3-none-any.whl (370 kB)\n",
      "Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Using cached oqpy-0.3.9-py3-none-any.whl (35 kB)\n",
      "Using cached amazon_braket_pennylane_plugin-1.33.6-py3-none-any.whl (41 kB)\n",
      "Using cached amazon_braket_default_simulator-1.32.0-py3-none-any.whl (232 kB)\n",
      "Using cached antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
      "Using cached amazon_braket_schemas-1.27.0-py3-none-any.whl (152 kB)\n",
      "Using cached boto3-1.42.0-py3-none-any.whl (140 kB)\n",
      "Using cached botocore-1.41.6-py3-none-any.whl (14.4 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached openpulse-1.0.1-py3-none-any.whl (537 kB)\n",
      "Using cached openqasm3-1.0.1-py3-none-any.whl (541 kB)\n",
      "Using cached pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
      "Using cached autoray-0.7.2-py3-none-any.whl (930 kB)\n",
      "Using cached pennylane_lightning-0.42.0-cp310-cp310-manylinux_2_28_x86_64.whl (2.6 MB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached scipy_openblas32-0.3.30.0.8-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached backports.entry_points_selectable-1.3.0-py3-none-any.whl (6.2 kB)\n",
      "Using cached boltons-25.0.0-py3-none-any.whl (194 kB)\n",
      "Using cached cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "Using cached llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: openqasm3, appdirs, antlr4-python3-runtime, typing-inspection, tomlkit, scipy-openblas32, pydantic-core, nest-asyncio, mypy-extensions, llvmlite, jmespath, cloudpickle, cachetools, boltons, backports.entry-points-selectable, backoff, autoray, autograd, annotated-types, pydantic, openpulse, numba, diastatic-malt, botocore, s3transfer, oqpy, amazon-braket-schemas, boto3, amazon-braket-default-simulator, amazon-braket-sdk, pennylane-lightning, pennylane, amazon-braket-pennylane-plugin\n",
      "\u001b[?25l\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/33\u001b[0m [openqasm3]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/33\u001b[0m [openqasm3]\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/33\u001b[0m [appdirs]\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/33\u001b[0m [tomlkit]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/33\u001b[0m [scipy-openblas32]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/33\u001b[0m [llvmlite]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/33\u001b[0m [jmespath]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/33\u001b[0m [boltons]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/33\u001b[0m [backoff]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/33\u001b[0m [autograd]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/33\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/33\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/33\u001b[0m [openpulse]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/33\u001b[0m [openpulse]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/33\u001b[0m [openpulse]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/33\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m23/33\u001b[0m [botocore]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m23/33\u001b[0m [botocore]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m23/33\u001b[0m [botocore]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m23/33\u001b[0m [botocore]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m26/33\u001b[0m [amazon-braket-schemas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m28/33\u001b[0m [amazon-braket-default-simulator]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m28/33\u001b[0m [amazon-braket-default-simulator]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m29/33\u001b[0m [amazon-braket-sdk]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m30/33\u001b[0m [pennylane-lightning]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m31/33\u001b[0m [pennylane]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/33\u001b[0m [amazon-braket-pennylane-plugin]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed amazon-braket-default-simulator-1.32.0 amazon-braket-pennylane-plugin-1.33.6 amazon-braket-schemas-1.27.0 amazon-braket-sdk-1.106.1 annotated-types-0.7.0 antlr4-python3-runtime-4.13.2 appdirs-1.4.4 autograd-1.8.0 autoray-0.7.2 backoff-2.2.1 backports.entry-points-selectable-1.3.0 boltons-25.0.0 boto3-1.42.0 botocore-1.41.6 cachetools-6.2.2 cloudpickle-2.2.1 diastatic-malt-2.15.2 jmespath-1.0.1 llvmlite-0.45.1 mypy-extensions-1.1.0 nest-asyncio-1.6.0 numba-0.62.1 openpulse-1.0.1 openqasm3-1.0.1 oqpy-0.3.9 pennylane-0.42.3 pennylane-lightning-0.42.0 pydantic-2.12.5 pydantic-core-2.41.5 s3transfer-0.16.0 scipy-openblas32-0.3.30.0.8 tomlkit-0.13.3 typing-inspection-0.4.2\n",
      "\n",
      "Collecting ipykernel\n",
      "  Using cached ipykernel-7.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Using cached comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Using cached debugpy-1.8.17-cp310-cp310-manylinux_2_34_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Using cached ipython-8.37.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (5.9.1)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Using cached matplotlib_inline-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting exceptiongroup (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Collecting stack_data (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.15.0)\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Using cached wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Using cached parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached ipykernel-7.1.0-py3-none-any.whl (117 kB)\n",
      "Using cached comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Using cached debugpy-1.8.17-cp310-cp310-manylinux_2_34_x86_64.whl (3.1 MB)\n",
      "Using cached ipython-8.37.0-py3-none-any.whl (831 kB)\n",
      "Using cached prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
      "Using cached matplotlib_inline-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-3.0.1-py3-none-any.whl (27 kB)\n",
      "Using cached executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
      "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Using cached wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pure-eval, ptyprocess, wcwidth, pexpect, parso, matplotlib-inline, executing, exceptiongroup, decorator, debugpy, comm, asttokens, stack_data, prompt_toolkit, jedi, ipython, ipykernel\n",
      "\u001b[?25l\n",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [pexpect]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [debugpy]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/17\u001b[0m [asttokens]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m13/17\u001b[0m [prompt_toolkit]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m14/17\u001b[0m [jedi]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m14/17\u001b[0m [jedi]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m14/17\u001b[0m [jedi]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m15/17\u001b[0m [ipython]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m15/17\u001b[0m [ipython]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m15/17\u001b[0m [ipython]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m16/17\u001b[0m [ipykernel]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [ipykernel]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed asttokens-3.0.1 comm-0.2.3 debugpy-1.8.17 decorator-5.2.1 exceptiongroup-1.3.1 executing-2.2.1 ipykernel-7.1.0 ipython-8.37.0 jedi-0.19.2 matplotlib-inline-0.2.1 parso-0.8.5 pexpect-4.9.0 prompt_toolkit-3.0.52 ptyprocess-0.7.0 pure-eval-0.2.3 stack_data-0.6.3 wcwidth-0.2.14\n",
      "\n",
      "Installed kernelspec torch_env in /home/ec2-user/.local/share/jupyter/kernels/torch_env\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda create -n torch_env python=3.10 -y --quiet\n",
    "# !conda run -n torch_env pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "!conda run -n torch_env pip install torch==2.3.0 torchtext==0.18.0 torchquantum\n",
    "\n",
    "!conda run -n torch_env pip install amazon-braket-sdk amazon-braket-pennylane-plugin\n",
    "!conda run -n torch_env pip install ipykernel\n",
    "!conda run -n torch_env python -m ipykernel install --user --name torch_env --display-name \"PyTorch Env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6cb612-40c4-47c4-8b97-b4d5e825006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/torch_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117faf39-60ca-47ff-b4da-d5f10de37e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ! /home/ec2-user/anaconda3/envs/Braket/bin/python -m pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a584aef-8bfb-4638-9531-b07ab3ced030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quixer_braket_sdk.py\n",
    "import itertools\n",
    "import math\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.types import Device\n",
    "\n",
    "from braket.circuits import Circuit\n",
    "from braket.devices import LocalSimulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcfae8a-d8d1-46c8-979e-7ae91ac7658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1) Ansatz 14 -> Braket Circuit (numeric params)\n",
    "# -------------------------\n",
    "def ansatz_14_braket_circuit(n_qubits: int, params: torch.Tensor, layers: int = 1) -> Circuit:\n",
    "    \"\"\"\n",
    "    Build a Braket Circuit implementing ansatz 14 using *numeric* params.\n",
    "    params: 1D tensor length = 4 * n_qubits * layers\n",
    "    \"\"\"\n",
    "    if params.numel() != 4 * n_qubits * layers:\n",
    "        raise ValueError(f\"params length must be {4*n_qubits*layers}, got {params.numel()}\")\n",
    "\n",
    "    circ = Circuit()\n",
    "    p_iter = iter(params.tolist())\n",
    "\n",
    "    for _ in range(layers):\n",
    "        # First RY layer\n",
    "        for q in range(n_qubits):\n",
    "            angle = next(p_iter)\n",
    "            circ.ry(q, angle)\n",
    "\n",
    "        # First CRX layer (reverse order)\n",
    "        # Decompose CRX(control,target,theta) as:\n",
    "        # Rx(target, theta/2) - CNOT(control,target) - Rx(target, -theta/2) - CNOT(control,target)\n",
    "        for i in range(n_qubits - 1, -1, -1):\n",
    "            control = i\n",
    "            target = (i + 1) % n_qubits\n",
    "            theta = next(p_iter)\n",
    "            circ.rx(target, theta / 2.0)\n",
    "            circ.cnot(control, target)\n",
    "            circ.rx(target, -theta / 2.0)\n",
    "            circ.cnot(control, target)\n",
    "\n",
    "        # Second RY layer\n",
    "        for q in range(n_qubits):\n",
    "            angle = next(p_iter)\n",
    "            circ.ry(q, angle)\n",
    "\n",
    "        # Second CRX layer (different neighbor direction)\n",
    "        order = [n_qubits - 1] + list(range(n_qubits - 1))\n",
    "        for i in order:\n",
    "            control = i\n",
    "            target = (i - 1) % n_qubits\n",
    "            theta = next(p_iter)\n",
    "            circ.rx(target, theta / 2.0)\n",
    "            circ.cnot(control, target)\n",
    "            circ.rx(target, -theta / 2.0)\n",
    "            circ.cnot(control, target)\n",
    "\n",
    "    return circ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac88bf54-afb7-41d9-8c1f-11034762cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Simulator helpers: statevector and unitary construction with caching\n",
    "# -------------------------\n",
    "class BraketUnitaryCache:\n",
    "    \"\"\"\n",
    "    Cache mapping (n_qubits, params_tuple, layers) -> torch.Tensor (dim x dim complex)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._cache: Dict[Tuple[int, Tuple[float, ...], int], torch.Tensor] = {}\n",
    "\n",
    "    def get(self, n_qubits: int, params: torch.Tensor, layers: int):\n",
    "        key = (n_qubits, tuple(float(x) for x in params.tolist()), layers)\n",
    "        return self._cache.get(key, None)\n",
    "\n",
    "    def set(self, n_qubits: int, params: torch.Tensor, layers: int, U: torch.Tensor):\n",
    "        key = (n_qubits, tuple(float(x) for x in params.tolist()), layers)\n",
    "        self._cache[key] = U\n",
    "\n",
    "\n",
    "_unitary_cache = BraketUnitaryCache()\n",
    "\n",
    "\n",
    "def circuit_statevector(circ: Circuit, n_qubits: int, device: LocalSimulator) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run a Braket circuit (ending with state_vector() result type) on the LocalSimulator\n",
    "    and return the resulting statevector as a numpy complex array of shape (2**n_qubits,).\n",
    "    \"\"\"\n",
    "    # Append the state_vector result request\n",
    "    circ_with_sv = circ.state_vector()\n",
    "    # shots=0 requests exact statevector from LocalSimulator\n",
    "    task = device.run(circ_with_sv, shots=0)\n",
    "    result = task.result()\n",
    "    # Braket returns result_types list whose first entry was the StateVector with `.value`\n",
    "    # This access pattern is consistent with SDK examples: result.result_types[0].value\n",
    "    sv = np.array(result.result_types[0].value, dtype=np.complex128)\n",
    "    return sv\n",
    "\n",
    "\n",
    "def circuit_unitary(circ: Circuit, n_qubits: int, device: LocalSimulator, cache: BraketUnitaryCache = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Construct the numeric unitary for a numeric circuit by running it on each computational basis input.\n",
    "    Returns a torch.complex64 tensor shape (dim, dim).\n",
    "    Uses caching if provided.\n",
    "    \"\"\"\n",
    "    dim = 2**n_qubits\n",
    "    cols = []\n",
    "    for k in range(dim):\n",
    "        prep = Circuit()\n",
    "        # prepare |k> by applying X on qubits with bit=1\n",
    "        for q in range(n_qubits):\n",
    "            if ((k >> q) & 1) == 1:\n",
    "                prep.x(q)\n",
    "        total = prep + circ\n",
    "        sv = circuit_statevector(total, n_qubits, device)\n",
    "        cols.append(sv)\n",
    "    U = np.column_stack(cols)  # shape (dim, dim)\n",
    "    return torch.tensor(U, dtype=torch.complex64)\n",
    "\n",
    "\n",
    "def get_or_build_unitary(n_qubits: int, params: torch.Tensor, layers: int, device: LocalSimulator, cache: BraketUnitaryCache):\n",
    "    \"\"\"\n",
    "    Return cached unitary or build and cache it.\n",
    "    \"\"\"\n",
    "    if cache is not None:\n",
    "        cached = cache.get(n_qubits, params, layers)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "\n",
    "    circ = ansatz_14_braket_circuit(n_qubits, params, layers=layers)\n",
    "    U = circuit_unitary(circ, n_qubits, device, cache=cache)\n",
    "\n",
    "    if cache is not None:\n",
    "        cache.set(n_qubits, params, layers, U)\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5389a1-1f6c-425c-a1cd-ac1fb82ceb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) LCU & QSVT implementations (classical simulation)\n",
    "# -------------------------\n",
    "# def apply_linear_combination_of_unitaries_braket(\n",
    "#     initial_states: torch.Tensor,  # [batch, dim]\n",
    "#     pqc_parameters: torch.Tensor,  # [batch, n_tokens, n_pqc_params]\n",
    "#     n_qubits: int,\n",
    "#     lcu_coefficients: torch.Tensor,  # [n_tokens] complex\n",
    "#     device: LocalSimulator,\n",
    "#     layers: int = 1,\n",
    "#     cache: BraketUnitaryCache = None,\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Apply LCU: sum_w b_w U(w) |psi>  where U(w) is unitary given by PQC params for token w.\n",
    "#     Returns [batch, dim] complex tensor.\n",
    "#     \"\"\"\n",
    "\n",
    "#     batch = initial_states.shape[0]\n",
    "#     n_tokens = pqc_parameters.shape[1]\n",
    "#     dim = 2**n_qubits\n",
    "\n",
    "#     # Flatten parameter sets across batch x tokens -> (batch*n_tokens, n_params)\n",
    "#     flat_params = pqc_parameters.view(-1, pqc_parameters.shape[-1])\n",
    "\n",
    "#     # Build or fetch unitaries for each flattened param vector\n",
    "#     unitaries = []\n",
    "#     for params_vec in flat_params:\n",
    "#         U = get_or_build_unitary(n_qubits, params_vec, layers, device, cache)\n",
    "#         unitaries.append(U)\n",
    "#     # stack and reshape to [batch, n_tokens, dim, dim]\n",
    "#     unitaries = torch.stack(unitaries, dim=0).view(batch, n_tokens, dim, dim)\n",
    "\n",
    "#     # Expand initial states to [batch, n_tokens, dim]\n",
    "#     expanded = initial_states.unsqueeze(1).expand(-1, n_tokens, -1)\n",
    "#     # apply unitaries: [batch, n_tokens, dim] = einsum btij,btj->bti\n",
    "#     evolved = torch.einsum(\"btij,btj->bti\", unitaries, expanded)\n",
    "\n",
    "#     # ensure lcu_coefficients complex dtype and shape [n_tokens]\n",
    "#     lcu_coeffs = lcu_coefficients.to(torch.complex64).view(1, n_tokens, 1)\n",
    "#     weighted = evolved * lcu_coeffs\n",
    "#     summed = weighted.sum(dim=1)  # [batch, dim]\n",
    "#     return summed\n",
    "\n",
    "def apply_linear_combination_of_unitaries_braket(\n",
    "    initial_states: torch.Tensor,       # [batch, dim]\n",
    "    pqc_parameters: torch.Tensor,       # [batch, n_tokens, n_pqc_params] or [batch, n_pqc_params]\n",
    "    n_qubits: int,\n",
    "    lcu_coefficients: torch.Tensor,     # either [n_tokens] or [batch] complex\n",
    "    device: LocalSimulator,\n",
    "    layers: int = 1,\n",
    "    cache: BraketUnitaryCache = None,\n",
    ") -> torch.Tensor:\n",
    "    batch, dim = initial_states.shape\n",
    "\n",
    "    # Ensure pqc_parameters has 3 dims: [batch, n_tokens, n_params]\n",
    "    if pqc_parameters.ndim == 2:\n",
    "        pqc_parameters = pqc_parameters.unsqueeze(1)  # [batch, 1, n_params]\n",
    "\n",
    "    batch_check, n_tokens, n_params = pqc_parameters.shape\n",
    "    assert batch_check == batch, \"Batch size mismatch\"\n",
    "\n",
    "    # Flatten parameters for unitary generation\n",
    "    flat_params = pqc_parameters.reshape(-1, n_params)  # [batch*n_tokens, n_params]\n",
    "\n",
    "    # Generate unitaries\n",
    "    unitaries = [get_or_build_unitary(n_qubits, params, layers, device, cache) \n",
    "                 for params in flat_params]\n",
    "    unitaries = torch.stack(unitaries, dim=0).reshape(batch, n_tokens, 2**n_qubits, 2**n_qubits)\n",
    "\n",
    "    # Expand initial states to [batch, n_tokens, dim]\n",
    "    expanded_states = initial_states.unsqueeze(1).expand(batch, n_tokens, 2**n_qubits)\n",
    "\n",
    "    # Apply unitaries\n",
    "    evolved_states = torch.einsum(\"btij,btj->bti\", unitaries, expanded_states)\n",
    "\n",
    "    # Handle lcu_coefficients: either [n_tokens] or [batch]\n",
    "    if lcu_coefficients.numel() == n_tokens:\n",
    "        lcu_coeffs = lcu_coefficients.to(torch.complex64).view(1, n_tokens, 1)\n",
    "    elif lcu_coefficients.numel() == batch:\n",
    "        lcu_coeffs = lcu_coefficients.to(torch.complex64).view(batch, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected lcu_coefficients size {lcu_coefficients.shape}\")\n",
    "\n",
    "    weighted_states = evolved_states * lcu_coeffs\n",
    "    summed_states = weighted_states.sum(dim=1)  # sum over tokens\n",
    "    return summed_states\n",
    "\n",
    "\n",
    "def apply_qsvt_and_lcu_braket(\n",
    "    initial_states: torch.Tensor,\n",
    "    pqc_parameters: torch.Tensor,\n",
    "    n_qubits: int,\n",
    "    lcu_coefficients: torch.Tensor,\n",
    "    qsvt_polynomial_coefficients: torch.Tensor,\n",
    "    device: LocalSimulator,\n",
    "    layers: int = 1,\n",
    "    cache: BraketUnitaryCache = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply polynomial via repeated LCU application (classical-simulated QSVT).\n",
    "    \"\"\"\n",
    "    accumulated_state = qsvt_polynomial_coefficients[0] * initial_states\n",
    "    monomial_state = initial_states\n",
    "    for c in qsvt_polynomial_coefficients[1:]:\n",
    "        monomial_state = apply_linear_combination_of_unitaries_braket(\n",
    "            monomial_state,\n",
    "            pqc_parameters,\n",
    "            n_qubits,\n",
    "            lcu_coefficients,\n",
    "            device,\n",
    "            layers=layers,\n",
    "            cache=cache,\n",
    "        )\n",
    "        accumulated_state = accumulated_state + c * monomial_state\n",
    "\n",
    "    norm = torch.linalg.vector_norm(qsvt_polynomial_coefficients, ord=1)\n",
    "    return accumulated_state / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5efd4f-a583-4b70-824f-8bc7684a0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Pauli expectation helpers (from statevector)\n",
    "# -------------------------\n",
    "# def single_qubit_reduced_density(statevec: torch.Tensor, n_qubits: int, target: int) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Compute reduced 2x2 density matrix for a single qubit `target` by reshaping and tracing out others.\n",
    "#     statevec: [dim] complex torch tensor\n",
    "#     \"\"\"\n",
    "#     dim = 2**n_qubits\n",
    "#     rho = torch.outer(statevec, torch.conj(statevec))  # [dim, dim]\n",
    "#     # reshape to (2,)*n_qubits x (2,)*n_qubits and permute target to front\n",
    "#     shape = [2] * (2 * n_qubits)\n",
    "#     rho_reshaped = rho.view(*shape)\n",
    "#     axes = list(range(n_qubits))\n",
    "#     perm = [target] + [i for i in axes if i != target]\n",
    "#     perm_full = perm + [p + n_qubits for p in perm]\n",
    "#     rho_perm = rho_reshaped.permute(*perm_full)\n",
    "#     rho2 = rho_perm.contiguous().view(2, 2, -1, -1)\n",
    "#     reduced = torch.einsum(\"aabb->ab\", rho2)  # trace out the other systems\n",
    "#     return reduced\n",
    "\n",
    "def single_qubit_reduced_density(statevec: torch.Tensor, n_qubits: int, target: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute reduced 2x2 density matrix for a single qubit `target` by tracing out other qubits.\n",
    "    \"\"\"\n",
    "    # Full density matrix\n",
    "    rho = torch.outer(statevec, torch.conj(statevec))  # [2^n, 2^n]\n",
    "\n",
    "    # Reshape to (2,2,...,2) for n_qubits input & output axes\n",
    "    shape = [2] * (2 * n_qubits)\n",
    "    rho_reshaped = rho.view(*shape)\n",
    "\n",
    "    # Permute target qubit to the front for both input/output axes\n",
    "    axes = list(range(n_qubits))\n",
    "    perm = [target] + [i for i in axes if i != target]  # input axes\n",
    "    perm_full = perm + [p + n_qubits for p in perm]     # output axes\n",
    "    rho_perm = rho_reshaped.permute(*perm_full)\n",
    "\n",
    "    # Compute dimensions of \"rest\" qubits\n",
    "    rest_dim = 2**(n_qubits - 1)\n",
    "    rho2 = rho_perm.contiguous().view(2, rest_dim, 2, rest_dim)\n",
    "\n",
    "    # Trace out other qubits\n",
    "    reduced = torch.einsum(\"abab->ab\", rho2)\n",
    "    return reduced\n",
    "\n",
    "\n",
    "\n",
    "def measure_all_x_y_z_from_statevector(states: torch.Tensor, n_qubits: int, device: Device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    states: [batch, dim] complex\n",
    "    returns: [batch, 3*n_qubits] real expectations (order: for each qubit X,Y,Z)\n",
    "    \"\"\"\n",
    "    sx = torch.tensor([[0, 1], [1, 0]], dtype=torch.complex64, device=device)\n",
    "    sy = torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex64, device=device)\n",
    "    sz = torch.tensor([[1, 0], [0, -1]], dtype=torch.complex64, device=device)\n",
    "\n",
    "    batch = states.shape[0]\n",
    "    res = []\n",
    "    for b in range(batch):\n",
    "        state = states[b]\n",
    "        evs = []\n",
    "        for q in range(n_qubits):\n",
    "            reduced = single_qubit_reduced_density(state, n_qubits, q)\n",
    "            ev_x = torch.trace(reduced @ sx).real\n",
    "            ev_y = torch.trace(reduced @ sy).real\n",
    "            ev_z = torch.trace(reduced @ sz).real\n",
    "            evs.extend([ev_x, ev_y, ev_z])\n",
    "        res.append(torch.stack(evs))\n",
    "    return torch.stack(res, dim=0)  # [batch, 3*n_qubits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd91404-4aaf-46e3-8f18-fcb54ae7d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5) QuixerBraket model (torch.nn.Module)\n",
    "# -------------------------\n",
    "class Quixer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_qubits: int,\n",
    "        n_tokens: int,\n",
    "        qsvt_polynomial_degree: int,\n",
    "        n_ansatz_layers: int,\n",
    "        vocabulary_size: int,\n",
    "        embedding_dimension: int,\n",
    "        dropout: float,\n",
    "        batch_size: int,\n",
    "        device: Device,\n",
    "        braket_device: LocalSimulator = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_qubits = n_qubits\n",
    "        if qsvt_polynomial_degree <= 0:\n",
    "            raise ValueError(\"qsvt_polynomial_degree must be > 0\")\n",
    "        self.degree = qsvt_polynomial_degree\n",
    "        self.device = device\n",
    "        self.n_ansatz_layers = n_ansatz_layers\n",
    "\n",
    "        if n_tokens == 0:\n",
    "            raise ValueError(\"n_tokens must be non-zero\")\n",
    "        self.n_ctrl_qubits = int(math.log2(n_tokens))\n",
    "\n",
    "        self.n_pqc_parameters = 4 * n_qubits * n_ansatz_layers\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        # Embedding & angle mapper\n",
    "        self.embedding = torch.nn.Embedding(vocabulary_size, self.embedding_dimension)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.embedding_to_angles = torch.nn.Linear(self.embedding_dimension, self.n_pqc_parameters)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Braket device (LocalSimulator by default)\n",
    "        self.braket_device = braket_device if braket_device is not None else LocalSimulator()\n",
    "\n",
    "        # Parameters (trainable)\n",
    "        self.qsvt_polynomial_coefficients = torch.nn.Parameter(torch.rand(self.degree + 1))\n",
    "        # complex LCU coefficients: store as two real tensors and combine when needed,\n",
    "        # but for simplicity store as complex dtype parameter (PyTorch supports it).\n",
    "        self.lcu_coefficients = torch.nn.Parameter(torch.rand(n_tokens, dtype=torch.complex64))\n",
    "\n",
    "        # Final PQC parameters\n",
    "        self.quantum_feedforward_parameters = torch.nn.Parameter(torch.rand(self.n_pqc_parameters))\n",
    "\n",
    "        self.nr_of_measurements = 3 * n_qubits\n",
    "        self.output_feedforward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.nr_of_measurements, self.embedding_dimension),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.embedding_dimension, vocabulary_size),\n",
    "        )\n",
    "\n",
    "        # unitary cache\n",
    "        self.unitary_cache = _unitary_cache\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(2**self.n_qubits, vocabulary_size)\n",
    "\n",
    "    # def forward(self, x: torch.Tensor):\n",
    "    #     \"\"\"\n",
    "    #     x: [batch, n_tokens] integer token ids\n",
    "    #     returns: (output_logits [batch, vocab_size], mean_postselection_prob scalar)\n",
    "    #     \"\"\"\n",
    "    #     batch_size = x.shape[0]\n",
    "\n",
    "    #     # LCU coefficients repeated per batch and normalized (1-norm)\n",
    "    #     lcu_coefficients = self.lcu_coefficients.repeat(batch_size, 1)\n",
    "    #     lcu_coefficients = torch.nn.functional.normalize(lcu_coefficients, p=1, dim=1)\n",
    "\n",
    "    #     # Get embeddings & PQC angles: [batch, n_tokens, n_pqc_parameters]\n",
    "    #     x_emb = self.embedding(x)\n",
    "    #     pqc_angles = self.embedding_to_angles(self.dropout(x_emb))\n",
    "\n",
    "    #     # Initial |0> states for each batch\n",
    "    #     initial_states = torch.zeros(batch_size, 2**self.n_qubits, dtype=torch.complex64, device=self.device)\n",
    "    #     initial_states[:, 0] = 1.0 + 0.0j\n",
    "\n",
    "    #     # Apply QSVT + LCU (classical simulation using Braket)\n",
    "    #     # Note: LCU coefficients in apply_qsvt_and_lcu_braket expect shape [n_tokens] complex;\n",
    "    #     # in our implementation lcu_coefficients is [batch, n_tokens] so pick first row since identical per batch\n",
    "        # qsvt_lcu_state = apply_qsvt_and_lcu_braket(\n",
    "        #     initial_states,\n",
    "        #     pqc_angles,\n",
    "        #     self.n_qubits,\n",
    "        #     lcu_coefficients[0],  # assume same across batch (like your original code)\n",
    "        #     self.qsvt_polynomial_coefficients,\n",
    "        #     self.braket_device,\n",
    "        #     layers=self.n_ansatz_layers,\n",
    "        #     cache=self.unitary_cache,\n",
    "        # )\n",
    "        \n",
    "        # # Normalize and set as current state for final feedforward PQC application\n",
    "        # norms = torch.linalg.vector_norm(qsvt_lcu_state, dim=-1, keepdim=True)\n",
    "        # normalized_states = qsvt_lcu_state / (norms + 1e-12)\n",
    "\n",
    "        # # Build unitary for final feedforward PQC (single param vector)\n",
    "        # feed_U = get_or_build_unitary(self.n_qubits, self.quantum_feedforward_parameters, 1, self.braket_device, self.unitary_cache)\n",
    "        # # Apply (matrix-vector): feed_U @ psi  (feed_U has shape [dim, dim])\n",
    "        # # Note on orientation: our `circuit_unitary` built columns as U |k> so standard matrix.\n",
    "        # final_states = torch.einsum(\"ij,bj->bi\", feed_U, normalized_states)\n",
    "\n",
    "        # # Measure expectations (X,Y,Z per qubit) from final_states\n",
    "        # expectation_values = measure_all_x_y_z_from_statevector(final_states, self.n_qubits, self.device)\n",
    "\n",
    "        # output_logits = self.output_feedforward(expectation_values)\n",
    "\n",
    "        # # Postselection probabilities (norms before normalization)\n",
    "        # final_probabilities = norms.squeeze(-1)  # [batch]\n",
    "        # return output_logits, torch.mean(final_probabilities)\n",
    "\n",
    "    # def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    #     \"\"\"\n",
    "    #     Forward pass for Quixer model.\n",
    "        \n",
    "    #     Args:\n",
    "    #         x: [batch, window_size] of token indices\n",
    "    \n",
    "    #     Returns:\n",
    "    #         logits: [batch, vocab_size]\n",
    "    #         mean_postselection_prob: scalar, average norm after LCU\n",
    "    #     \"\"\"\n",
    "    #     # Ensure x is 2D: [batch, seq_len]\n",
    "    #     if x.dim() == 1:\n",
    "    #         x = x.unsqueeze(0)\n",
    "    #     batch, seq_len = x.shape\n",
    "    \n",
    "    #     # 1. Embeddings\n",
    "    #     x_emb = self.embedding(x)  # [batch, seq_len, embedding_dim]\n",
    "    #     pqc_angles = self.embedding_to_angles(self.dropout(x_emb))  # [batch, seq_len, n_pqc_params]\n",
    "    \n",
    "    #     # 2. Prepare initial quantum states\n",
    "    #     dim = 2 ** self.n_qubits\n",
    "    #     initial_states = torch.zeros(batch, dim, dtype=torch.complex64, device=x.device)\n",
    "    #     initial_states[:, 0] = 1.0 + 0.0j  # |0> for each batch\n",
    "\n",
    "    #     # 3. LCU coefficients\n",
    "    #     # Make 2D: [1, n_tokens] for normalization, then expand to batch\n",
    "    #     lcu_coeffs = self.lcu_coefficients.unsqueeze(0)  # [1, n_tokens]\n",
    "    #     lcu_coeffs = torch.nn.functional.normalize(lcu_coeffs, p=1, dim=1)  # normalize across tokens\n",
    "    #     lcu_coeffs = lcu_coeffs.expand(batch, -1)  # [batch, n_tokens]\n",
    "    \n",
    "    #     # 4. Apply QSVT + LCU (classical simulation)\n",
    "    #     qsvt_lcu_state = apply_qsvt_and_lcu_braket(\n",
    "    #         initial_states,\n",
    "    #         pqc_angles,\n",
    "    #         self.n_qubits,\n",
    "    #         lcu_coeffs,\n",
    "    #         self.qsvt_polynomial_coefficients,\n",
    "    #         self.braket_device,\n",
    "    #         layers=self.n_ansatz_layers,\n",
    "    #         cache=self.unitary_cache,\n",
    "    #     )\n",
    "    \n",
    "    #     # 5. Normalize LCU-evolved states and compute average norm\n",
    "    #     norms = torch.linalg.vector_norm(qsvt_lcu_state, dim=-1, keepdim=True)  # [batch, 1]\n",
    "    #     normalized_states = qsvt_lcu_state / (norms + 1e-12)\n",
    "    #     mean_postselection_prob = norms.mean()\n",
    "\n",
    "    #     # 6. Apply final feedforward PQC\n",
    "    #     feed_U = get_or_build_unitary(\n",
    "    #         self.n_qubits,\n",
    "    #         self.quantum_feedforward_parameters,\n",
    "    #         1,\n",
    "    #         self.braket_device,\n",
    "    #         self.unitary_cache,\n",
    "    #     )  # [dim, dim]\n",
    "    \n",
    "    #     final_states = torch.einsum(\"ij,bj->bi\", feed_U, normalized_states)  # [batch, dim]\n",
    "    \n",
    "    #     # 7. Classical projection to vocab logits\n",
    "    #     # logits = self.output_layer(final_states)  # [batch, vocab_size]\n",
    "    #     logits = self.output_layer(final_states.real)\n",
    "    \n",
    "    #     return logits, mean_postselection_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for Quixer model.\n",
    "    \n",
    "        Args:\n",
    "            x: [batch, seq_len] of token indices\n",
    "    \n",
    "        Returns:\n",
    "            logits: [batch, vocab_size]\n",
    "            mean_postselection_prob: scalar tensor\n",
    "        \"\"\"\n",
    "        batch = x.shape[0]\n",
    "    \n",
    "        # 1. Embedding lookup\n",
    "        x_emb = self.embedding(x)  # [batch, seq_len, embedding_dim]\n",
    "        pqc_angles = self.embedding_to_angles(self.dropout(x_emb))  # [batch, seq_len, n_pqc_params]\n",
    "    \n",
    "        # 2. Prepare initial quantum states |0>\n",
    "        dim = 2**self.n_qubits\n",
    "        initial_states = torch.zeros(batch, dim, dtype=torch.complex64, device=x.device)\n",
    "        initial_states[:, 0] = 1.0 + 0.0j\n",
    "    \n",
    "        # 3. Compute LCU coefficients\n",
    "        # If single set per model, expand to batch\n",
    "        lcu_coeffs = torch.nn.functional.normalize(\n",
    "            self.lcu_coefficients, p=1, dim=-1\n",
    "        )  # [n_tokens] or [1, n_tokens]\n",
    "        if lcu_coeffs.dim() == 1:\n",
    "            lcu_coeffs = lcu_coeffs.unsqueeze(0).expand(batch, -1)  # [batch, n_tokens]\n",
    "    \n",
    "        # 4. Apply QSVT + LCU\n",
    "        qsvt_lcu_state = apply_qsvt_and_lcu_braket(\n",
    "            initial_states,\n",
    "            pqc_angles,\n",
    "            self.n_qubits,\n",
    "            lcu_coeffs[0],  # assume same across batch\n",
    "            self.qsvt_polynomial_coefficients,\n",
    "            self.braket_device,\n",
    "            layers=self.n_ansatz_layers,\n",
    "            cache=self.unitary_cache,\n",
    "        )\n",
    "    \n",
    "        # 5. Normalize state\n",
    "        norms = torch.linalg.vector_norm(qsvt_lcu_state, dim=-1, keepdim=True)\n",
    "        normalized_states = qsvt_lcu_state / (norms + 1e-12)\n",
    "        mean_postselection_prob = norms.mean()\n",
    "    \n",
    "        # 6. Apply final feedforward PQC\n",
    "        feed_U = get_or_build_unitary(\n",
    "            self.n_qubits,\n",
    "            self.quantum_feedforward_parameters,\n",
    "            layers=self.n_ansatz_layers,\n",
    "            device=self.braket_device,\n",
    "            cache=self.unitary_cache,\n",
    "        )  # [dim, dim]\n",
    "    \n",
    "        # Apply to normalized states\n",
    "        final_states = torch.einsum(\"ij,bj->bi\", feed_U, normalized_states)  # [batch, dim]\n",
    "    \n",
    "        # 7. Classical projection to vocab logits\n",
    "        # Convert complex to real for nn.Linear\n",
    "        logits = self.output_layer(final_states.real)  # [batch, vocab_size]\n",
    "    \n",
    "        return logits, mean_postselection_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e20ec5-8014-4f72-ad83-c9e06489d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-0.0835, -0.4767, -0.4800, -0.4976, -0.5488,  0.7021,  0.6041,  0.0558,\n",
      "         -0.1709, -0.2785]], grad_fn=<AddmmBackward0>)\n",
      "mean postselection prob: tensor(0.7684, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Small example usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # small example: 2 qubits, 2 tokens, tiny vocab\n",
    "    n_qubits = 2\n",
    "    n_tokens = 2\n",
    "    qsvt_degree = 2\n",
    "    n_layers = 1\n",
    "    vocab = 10\n",
    "    emb_dim = 8\n",
    "    dropout = 0.0\n",
    "    batch_size = 1\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model = Quixer(\n",
    "        n_qubits=n_qubits,\n",
    "        n_tokens=n_tokens,\n",
    "        qsvt_polynomial_degree=qsvt_degree,\n",
    "        n_ansatz_layers=n_layers,\n",
    "        vocabulary_size=vocab,\n",
    "        embedding_dimension=emb_dim,\n",
    "        dropout=dropout,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # dummy input: batch of 1, sequence length n_tokens\n",
    "    x = torch.randint(0, vocab, (batch_size, n_tokens))\n",
    "    logits, mean_prob = model(x)\n",
    "    print(\"logits:\", logits)\n",
    "    print(\"mean postselection prob:\", mean_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40981bd7-feda-49e9-a45f-4044dd7d786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/torch_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df11127-9f27-41d7-b793-7441a84f8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2648d05-1c9c-45d6-82d6-119c2a8e282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip uninstall -y torchtext\n",
    "# !pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85f3d8f7-9f42-4262-8bbf-23582f1a29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install \"torchtext==0.17.1\"\n",
    "# !{sys.executable} -m pip install --force-reinstall \"torch==2.9.1\" \"torchtext==0.17.1\" --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# !conda run -n torch_env pip uninstall -y torch torchtext\n",
    "# !conda run -n torch_env pip uninstall -y torch torchtext\n",
    "# !conda run -n torch_env pip install torch==2.3.0 torchtext==0.18.0 amazon-braket-sdk torchquantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80eb9790-f553-474b-a31c-7b2710942fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (0.3.4)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.1.7-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-1.1.7-py3-none-any.whl (516 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, shellingham, pyarrow, propcache, multidict, hf-xet, h11, frozenlist, click, async-timeout, aiohappyeyeballs, yarl, typer-slim, pandas, httpcore, anyio, aiosignal, httpx, aiohttp, huggingface-hub, datasets\n",
      "\u001b[?25l\n",
      "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/23\u001b[0m [tzdata]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/23\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/23\u001b[0m [hf-xet]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/23\u001b[0m [typer-slim]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m15/23\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m17/23\u001b[0m [anyio]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m20/23\u001b[0m [aiohttp]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m21/23\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m21/23\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m22/23\u001b[0m [datasets]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [datasets]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 async-timeout-5.0.1 click-8.3.1 datasets-4.4.1 frozenlist-1.8.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.1.7 multidict-6.7.0 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 shellingham-1.5.4 typer-slim-0.20.0 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda run -n torch_env pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2cbf774-b859-4eac-9cae-a7f3e605cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata\n",
      "  Using cached torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torchdata) (2.5.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torchdata) (2.32.5)\n",
      "Requirement already satisfied: torch>=2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torchdata) (2.3.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from torch>=2->torchdata) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests->torchdata) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests->torchdata) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from requests->torchdata) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Using cached torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: torchdata\n",
      "Successfully installed torchdata-0.11.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchtext\n",
    "!conda run -n torch_env pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac34afe-6a85-48ac-8aa5-2894832516a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "########### Quixer/quixer/setuptraining.py\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Optional, Tuple, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.types import Device\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torchtext\n",
    "\n",
    "# from quixer.quixer_model import Quixer\n",
    "# from quixer.baseline_models import Transformer, LSTM, FNet\n",
    "from torchtext.datasets import PennTreebank\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "\n",
    "def epoch_time(start_time: float, end_time: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Computes time elapsed in minutes and seconds when given two UNIX timestamps\n",
    "    with the starting time and ending time.\n",
    "\n",
    "    Args:\n",
    "      start_time: Starting time as a UNIX timestamp.\n",
    "      end_time: End time as a UNIX timestamp.\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def batchify_s2s(\n",
    "    data: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    window_size: int,\n",
    "    pad_token_id: int,\n",
    "    device: Device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Takes in a sequence of token IDs as a torch tensor `data` and returns a torch tensor containing\n",
    "    the training data with shape `[number of batches + window_size, batch_size]`.\n",
    "\n",
    "    Each batch is represented by `window_size` contiguous rows in the returned tensor and\n",
    "    can be extracted using the `get_batch_s2s` function.\n",
    "\n",
    "    A sequence of pad tokens of length `window_size-1` is prepended to the data so as to\n",
    "    provide a context window for the first token.\n",
    "\n",
    "    Args:\n",
    "      data: A 1D torch tensor containing a sequence of token IDs.\n",
    "      batch_size: The number of sequences each batch should have.\n",
    "      window_size: How many tokens are considered in each context window (each of which is a sequence in the batch).\n",
    "      pad_token_id: The ID of the pad token, as supplied by the tokenizer.\n",
    "      device: Torch device the returned tensor is to be created on.\n",
    "\n",
    "    Returns:\n",
    "      Tensor containing data for each batch prepared for a next token prediction language\n",
    "      modelling task.\n",
    "    \"\"\"\n",
    "    batch_nr_of_elements = batch_size * window_size\n",
    "    nr_of_batches = (data.size(0) - 1) // batch_nr_of_elements\n",
    "\n",
    "    # Discard tokens at the end of the data that do not fill a whole batch\n",
    "    batched_data = (\n",
    "        data[: nr_of_batches * batch_nr_of_elements]\n",
    "        .view(batch_nr_of_elements, nr_of_batches)\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    # Data for the first batch\n",
    "    window_data = torch.cat(\n",
    "        (\n",
    "            # Adds a sequence of pad tokens of length `window_size-1`\n",
    "            # to provide a context window for the first token.\n",
    "            torch.full((window_size, 1), pad_token_id, device=device),\n",
    "            # Context for the first row of tokens in `batched_data`\n",
    "            batched_data[-window_size:, :-1],\n",
    "        ),\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    return torch.cat((window_data, batched_data))\n",
    "\n",
    "\n",
    "def get_batch_s2s(\n",
    "    source: torch.Tensor, i: int, window_size: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns the `i`th batch; expects one of the tensors returned by `setup_dataset`.\n",
    "\n",
    "    Args:\n",
    "      source: Tensor containing data.\n",
    "      i: Index of the batch.\n",
    "      window_size: Context window size.\n",
    "    Returns:\n",
    "      The `i`th batch.\n",
    "    \"\"\"\n",
    "    return source[i : i + window_size].T, source[i + window_size]\n",
    "\n",
    "\n",
    "def initialise_weights(model: torch.nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initialises model weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init_weights(m):\n",
    "        if type(m) == torch.nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, torch.nn.Embedding):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    model.apply(_init_weights)\n",
    "\n",
    "\n",
    "# def setup_dataset(\n",
    "#     device: Device, batch_size: int, window_size: int\n",
    "# ) -> Tuple[torchtext.vocab.Vocab, Tuple[torch.Tensor, torch.Tensor, torch.Tensor], int]:\n",
    "#     \"\"\"\n",
    "#     Downloads and tokenizes the Penn TreeBank dataset, and then sets it up for a\n",
    "#     next-word prediction task.\n",
    "\n",
    "#     Args:\n",
    "#       device: Device to store dataset on.\n",
    "#       batch_size: Size of the batches.\n",
    "#       window_size: Size of the context window.\n",
    "\n",
    "#     Returns:\n",
    "#       Vocabulary represented by a torchtext.vocab.Vocab instance along with\n",
    "#       three torch tensors containing the training, validation and test data.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Download dataset from the Hugging Face Hub / load dataset\n",
    "#     # raw_dset = load_dataset(\"ptb_text_only\")\n",
    "#     raw_dset = load_dataset(\"ptb_text_only\", \"penn_treebank\")\n",
    "\n",
    "#     # Get training data in PyArrow format\n",
    "#     train_iter = raw_dset[\"train\"].data[0]\n",
    "#     # Convert from arrow array to native Python list\n",
    "#     train_iter = [s.as_py() for s in train_iter]\n",
    "\n",
    "#     # Get torchtext tokenizer\n",
    "#     tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "#     vocab = build_vocab_from_iterator(\n",
    "#         map(tokenizer, train_iter), specials=[\"<pad>\", \"<unk>\", \"<eos>\"]\n",
    "#     )\n",
    "#     # Define unknown word as the default index to use\n",
    "#     vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "#     def data_process(raw_text_iter) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Converts raw text into a flat Tensor of token indices.\n",
    "#         \"\"\"\n",
    "#         data = [\n",
    "#             torch.tensor(vocab(tokenizer(item)) + [vocab[\"eos\"]], dtype=torch.long)\n",
    "#             for item in raw_text_iter\n",
    "#         ]\n",
    "#         return torch.cat(tuple(filter(lambda t: t.numel() > 1, data))).to(device)\n",
    "\n",
    "#     # Convert from arrow arrays to native Python lists\n",
    "#     train_sents = [s.as_py() for s in raw_dset[\"train\"].data[0]]\n",
    "#     val_sents = [s.as_py() for s in raw_dset[\"validation\"].data[0]]\n",
    "#     test_sents = [s.as_py() for s in raw_dset[\"test\"].data[0]]\n",
    "\n",
    "#     # Flatten datasets into one long tokenised string each\n",
    "#     train_flat = data_process(train_sents)\n",
    "#     val_flat = data_process(val_sents)\n",
    "#     test_flat = data_process(test_sents)\n",
    "\n",
    "#     # Get padding token\n",
    "#     PAD_TOKEN = vocab[\"<pad>\"]\n",
    "\n",
    "#     # Prepare data for a next-token prediction language modelling task\n",
    "#     train_iter = batchify_s2s(train_flat, batch_size, window_size, PAD_TOKEN, device)\n",
    "#     val_iter = batchify_s2s(val_flat, batch_size, window_size, PAD_TOKEN, device)\n",
    "#     test_iter = batchify_s2s(test_flat, batch_size, window_size, PAD_TOKEN, device)\n",
    "\n",
    "#     return vocab, (train_iter, val_iter, test_iter), PAD_TOKEN\n",
    "\n",
    "# def setup_dataset(device, batch_size, window_size):\n",
    "#     # Load modern HuggingFace PTB\n",
    "#     # raw = load_dataset(\"ptb\")\n",
    "#     # raw = load_dataset(\"ptb_text_only\", \"penn_treebank\")\n",
    "#     raw = load_dataset(\"penn_treebank\")\n",
    "\n",
    "#     # Extract text\n",
    "#     train_texts = [x[\"sentence\"] for x in raw[\"train\"]]\n",
    "#     val_texts   = [x[\"sentence\"] for x in raw[\"validation\"]]\n",
    "#     test_texts  = [x[\"sentence\"] for x in raw[\"test\"]]\n",
    "\n",
    "#     # Tokenize simply by splitting\n",
    "#     train_tokens = [t.split() for t in train_texts]\n",
    "#     val_tokens   = [t.split() for t in val_texts]\n",
    "#     test_tokens  = [t.split() for t in test_texts]\n",
    "\n",
    "#     # Build vocab\n",
    "#     counter = Counter()\n",
    "#     for sent in train_tokens:\n",
    "#         counter.update(sent)\n",
    "\n",
    "#     vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "#     for tok, _ in counter.most_common():\n",
    "#         vocab[tok] = len(vocab)\n",
    "\n",
    "#     PAD_TOK = vocab[\"<pad>\"]\n",
    "\n",
    "#     # Numericalize\n",
    "#     def numericalize(tokens):\n",
    "#         return torch.tensor([vocab.get(tok, 1) for tok in tokens], dtype=torch.long)\n",
    "\n",
    "#     train_tensors = [numericalize(t) for t in train_tokens]\n",
    "#     val_tensors   = [numericalize(t) for t in val_tokens]\n",
    "#     test_tensors  = [numericalize(t) for t in test_tokens]\n",
    "\n",
    "#     # Simple batching (same as legacy PTB examples)\n",
    "#     def make_batches(seqs):\n",
    "#         batches = []\n",
    "#         for seq in seqs:\n",
    "#             # chop into windows\n",
    "#             for i in range(0, len(seq) - window_size):\n",
    "#                 X = seq[i:i+window_size]\n",
    "#                 y = seq[i+1:i+window_size+1]\n",
    "#                 batches.append((X, y))\n",
    "#         return batches\n",
    "\n",
    "#     train_iter = make_batches(train_tensors)\n",
    "#     val_iter   = make_batches(val_tensors)\n",
    "#     test_iter  = make_batches(test_tensors)\n",
    "\n",
    "#     return vocab, (train_iter, val_iter, test_iter), PAD_TOK\n",
    "\n",
    "def setup_dataset(device, batch_size, window_size, ptb_dir=\"/home/ec2-user/SageMaker/quixer\"):\n",
    "\n",
    "    # Load raw PTB text from local files\n",
    "    def load_file(name):\n",
    "        path = os.path.join(ptb_dir, name)\n",
    "        with open(path, \"r\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    train_texts = load_file(\"ptb.train.txt\")\n",
    "    val_texts = load_file(\"ptb.valid.txt\")\n",
    "    test_texts = load_file(\"ptb.test.txt\")\n",
    "\n",
    "    # Tokenize simply by splitting\n",
    "    train_tokens = [t.split() for t in train_texts]\n",
    "    val_tokens   = [t.split() for t in val_texts]\n",
    "    test_tokens  = [t.split() for t in test_texts]\n",
    "\n",
    "    # Build vocab\n",
    "    counter = Counter()\n",
    "    for sent in train_tokens:\n",
    "        counter.update(sent)\n",
    "\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for tok, _ in counter.most_common():\n",
    "        vocab[tok] = len(vocab)\n",
    "\n",
    "    PAD_TOK = vocab[\"<pad>\"]\n",
    "\n",
    "    # Numericalize\n",
    "    def numericalize(tokens):\n",
    "        return torch.tensor([vocab.get(tok, 1) for tok in tokens], dtype=torch.long)\n",
    "\n",
    "    train_tensors = [numericalize(t) for t in train_tokens]\n",
    "    val_tensors   = [numericalize(t) for t in val_tokens]\n",
    "    test_tensors  = [numericalize(t) for t in test_tokens]\n",
    "\n",
    "    # Simple batching (same as legacy PTB examples)\n",
    "    def make_batches(seqs):\n",
    "        batches = []\n",
    "        for seq in seqs:\n",
    "            # chop into windows\n",
    "            for i in range(0, len(seq) - window_size):\n",
    "                X = seq[i:i+window_size]\n",
    "                y = seq[i+1:i+window_size+1]\n",
    "                batches.append((X, y))\n",
    "        return batches\n",
    "\n",
    "    train_iter = make_batches(train_tensors)\n",
    "    val_iter   = make_batches(val_tensors)\n",
    "    test_iter  = make_batches(test_tensors)\n",
    "\n",
    "    return vocab, (train_iter, val_iter, test_iter), PAD_TOK\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    hyperparams: dict[str, Any], device: Device, vocabulary_size: int\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Selects and creates model based on hyperparameters passed.\n",
    "\n",
    "    Args:\n",
    "      hyperparams: Model hyperparameters.\n",
    "      device: Device the model will be run on.\n",
    "      vocabulary_size: Size of the vocabulary.\n",
    "    Returns:\n",
    "      An instance of a torch model based on the hyperparameters passed.\n",
    "    \"\"\"\n",
    "    model_str = hyperparams[\"model\"]\n",
    "    model: torch.nn.Module\n",
    "    if model_str == \"Quixer\":\n",
    "        model = Quixer(\n",
    "            n_qubits=hyperparams[\"qubits\"],\n",
    "            n_tokens=hyperparams[\"window\"],\n",
    "            qsvt_polynomial_degree=hyperparams[\"layers\"],\n",
    "            n_ansatz_layers=hyperparams[\"ansatz_layers\"],\n",
    "            vocabulary_size=vocabulary_size,\n",
    "            embedding_dimension=hyperparams[\"dimension\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            batch_size=hyperparams[\"batch_size\"],\n",
    "            device=device,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized model: {model_str}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# def train_epoch(\n",
    "#     model: torch.nn.Module,\n",
    "#     iterator: torch.Tensor,\n",
    "#     optimizer: torch.optim.Optimizer,\n",
    "#     loss_function: _Loss,\n",
    "#     clip: float,\n",
    "#     scheduler: Optional[torch.optim.lr_scheduler.LRScheduler],\n",
    "#     window_size: int,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Runs training loop for one epoch.\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     # n_batches = iterator.shape[0] - window_size\n",
    "#     n_batches = len(iterator) - window_size\n",
    "\n",
    "#     idxs = list(range(n_batches))\n",
    "#     random.shuffle(idxs)\n",
    "\n",
    "#     for ctr, batch_idx in tqdm(enumerate(idxs), total=n_batches):\n",
    "#         x, y = get_batch_s2s(iterator, batch_idx, window_size)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         yhat, norm_avg = model(x)\n",
    "\n",
    "#         loss = loss_function(yhat, y)\n",
    "#         loss.backward()\n",
    "\n",
    "#         if clip:\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         if scheduler:\n",
    "#             scheduler.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     return epoch_loss / n_batches\n",
    "\n",
    "# def train_epoch(\n",
    "#     model: torch.nn.Module,\n",
    "#     iterator,\n",
    "#     optimizer: torch.optim.Optimizer,\n",
    "#     loss_function: _Loss,\n",
    "#     clip: float,\n",
    "#     scheduler: Optional[torch.optim.lr_scheduler.LRScheduler],\n",
    "#     window_size: int,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Runs training loop for one epoch.\n",
    "#     Supports either:\n",
    "#         - iterator as a 1D torch.Tensor (legacy sliding window)\n",
    "#         - iterator as a list of (X, y) tuples (batch-prepared)\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     # Determine if iterator is a tensor or list of tuples\n",
    "#     if isinstance(iterator, torch.Tensor):\n",
    "#         n_batches = len(iterator) - window_size\n",
    "#         idxs = list(range(n_batches))\n",
    "#         random.shuffle(idxs)\n",
    "\n",
    "#         for ctr, batch_idx in tqdm(enumerate(idxs), total=n_batches):\n",
    "#             x, y = get_batch_s2s(iterator, batch_idx, window_size)\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             yhat, norm_avg = model(x)\n",
    "#             loss = loss_function(yhat, y)\n",
    "#             loss.backward()\n",
    "\n",
    "#             if clip:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#             optimizer.step()\n",
    "#             if scheduler:\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#     elif isinstance(iterator, list):\n",
    "#         n_batches = len(iterator)\n",
    "#         idxs = list(range(n_batches))\n",
    "#         random.shuffle(idxs)\n",
    "\n",
    "#         for ctr, batch_idx in tqdm(enumerate(idxs), total=n_batches):\n",
    "#             # batch is already (X, y)\n",
    "#             X, Y = iterator[batch_idx]\n",
    "#             x = torch.tensor(X, dtype=torch.float32).T  # or torch.long if token IDs\n",
    "#             y = torch.tensor(Y, dtype=torch.float32)    # or torch.long if classification\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             yhat, norm_avg = model(x)\n",
    "#             loss = loss_function(yhat, y)\n",
    "#             loss.backward()\n",
    "\n",
    "#             if clip:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "#             optimizer.step()\n",
    "#             if scheduler:\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#     else:\n",
    "#         raise TypeError(f\"Unsupported iterator type: {type(iterator)}\")\n",
    "\n",
    "#     return epoch_loss / n_batches\n",
    "\n",
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    iterator: list[tuple[list[int], list[int]]],  # list of (X, Y) batches\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: _Loss,\n",
    "    clip: float,\n",
    "    scheduler: Optional[torch.optim.lr_scheduler.LRScheduler],\n",
    "    window_size: int,  # not used directly here since batches are pre-made\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs training loop for one epoch using pre-batched sequences.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model.\n",
    "        iterator: List of (X, Y) batches.\n",
    "        optimizer: Optimizer.\n",
    "        loss_function: Loss function.\n",
    "        clip: Gradient clipping value.\n",
    "        scheduler: Learning rate scheduler (optional).\n",
    "        window_size: sequence length (for legacy compatibility).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    idxs = list(range(n_batches))\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch_idx in tqdm(idxs, total=n_batches):\n",
    "        X, Y = iterator[batch_idx]\n",
    "\n",
    "        # Convert to tensors with correct dtype\n",
    "        x = torch.tensor(X, dtype=torch.long).to(device)  # for nn.Embedding\n",
    "        y = torch.tensor(Y, dtype=torch.long).to(device)  # for CrossEntropyLoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat, norm_avg = model(x)\n",
    "\n",
    "        loss = loss_function(yhat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "def evaluate(\n",
    "    model: torch.nn.Module,\n",
    "    data: torch.Tensor,\n",
    "    loss_function: _Loss,\n",
    "    window_size: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluates model on the supplied data.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = data.shape[0] - window_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in tqdm(range(n_batches)):\n",
    "            x, y = get_batch_s2s(data, batch_idx, window_size)\n",
    "\n",
    "            yhat, _ = model(x)\n",
    "\n",
    "            loss = loss_function(yhat, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def train_cycle(\n",
    "    model: torch.nn.Module,\n",
    "    hyperparams: dict[str, Any],\n",
    "    train_iter: torch.Tensor,\n",
    "    val_iter: torch.Tensor,\n",
    "    test_iter: torch.Tensor,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Run a training cycle.\n",
    "\n",
    "    Args:\n",
    "      model: The model to train.\n",
    "      hyperparams: The model hyperparameters.\n",
    "      train_iter: Tensor containing training data returned by `setup_dataset` function.\n",
    "      val_iter: Tensor containing validation data returned by `setup_dataset` function.\n",
    "      test_iter: Tensor containing test data returned by `setup_dataset` function.\n",
    "    \"\"\"\n",
    "\n",
    "    folder_path = Path(\"./trained_models\")\n",
    "    folder_path.mkdir(exist_ok=True, parents=True)\n",
    "    checkpoint_fpath = (\n",
    "        folder_path\n",
    "        / f\"q_transformer_lm_{hyperparams['model']}_{hyperparams['seed']}_{int(time.time())}.pt\"\n",
    "    )\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=hyperparams[\"lr\"],\n",
    "        weight_decay=hyperparams[\"wd\"],\n",
    "        eps=hyperparams[\"eps\"],\n",
    "    )\n",
    "\n",
    "    # Set up learning rate scheduler\n",
    "    scheduler = None\n",
    "    if hyperparams[\"lr_sched\"] == \"cos\":\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=hyperparams[\"restart_epochs\"]\n",
    "        )\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def _evaluate(iter: torch.Tensor):\n",
    "        return evaluate(model, iter, loss_function, hyperparams[\"window\"])\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    for epoch in range(hyperparams[\"epochs\"]):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_iter,\n",
    "            optimizer,\n",
    "            loss_function,\n",
    "            hyperparams[\"max_grad_norm\"],\n",
    "            scheduler,\n",
    "            hyperparams[\"window\"],\n",
    "        )\n",
    "\n",
    "        valid_loss = _evaluate(val_iter)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_fpath)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train ppl: {math.exp(train_loss)}\")\n",
    "        print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. ppl: {math.exp(valid_loss)}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_fpath))\n",
    "\n",
    "    valid_loss = _evaluate(val_iter)\n",
    "    test_loss = _evaluate(test_iter)\n",
    "\n",
    "    print(\"FINAL TRAINED MODEL STATS:\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. ppl: {math.exp(valid_loss)}\")\n",
    "    print(f\"\\t Test Loss: {test_loss:.3f} |  Test ppl: {math.exp(test_loss)}\")\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def seed(SEED: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets the seed for Python's random module, numpy's RNG and torch's RNG.\n",
    "\n",
    "    Args:\n",
    "      SEED: integer specifying the seed\n",
    "    \"\"\"\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def get_train_evaluate(device: Device) -> Callable:\n",
    "    \"\"\"\n",
    "    Returns a function that runs the training cycle on a specified torch device.\n",
    "\n",
    "    Args:\n",
    "      device: Torch device\n",
    "\n",
    "    Returns:\n",
    "      Callable taking in a set of parameters as a dict and returning the value of the validation loss\n",
    "      at the end of the training cycle.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_evaluate(parameterization: dict[str, Any]) -> float:\n",
    "        \"\"\"\n",
    "        Train the model and return the test loss.\n",
    "        \"\"\"\n",
    "\n",
    "        if \"seed\" not in parameterization:\n",
    "            parameterization[\"seed\"] = int.from_bytes(os.urandom(4), \"big\")\n",
    "\n",
    "        # seed(parameterization[\"seed\"])\n",
    "\n",
    "        torch.manual_seed(parameterization[\"seed\"])\n",
    "        random.seed(parameterization[\"seed\"])\n",
    "        np.random.seed(parameterization[\"seed\"])\n",
    "\n",
    "        vocab, (train_iter, val_iter, test_iter), PAD_TOK = setup_dataset(\n",
    "            device, parameterization[\"batch_size\"], parameterization[\"window\"]\n",
    "        )\n",
    "\n",
    "        model = create_model(parameterization, device, len(vocab))\n",
    "\n",
    "        initialise_weights(model)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        valid_loss = train_cycle(\n",
    "            model, parameterization, train_iter, val_iter, test_iter\n",
    "        )\n",
    "\n",
    "        return valid_loss\n",
    "\n",
    "    return train_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81122db9-03b3-41d8-8a49-a54270be7f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "0.18.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2f51c5-1abd-4c96-af9b-8d0b43278a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda run -n torch_env pip install argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d049e-878e-46b5-a0d2-a75cba8db746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "  0%|          | 0/2924 [00:00<?, ?it/s]/var/tmp/braket/ipykernel_219268/2846668155.py:479: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(X, dtype=torch.long).to(device)  # for nn.Embedding\n",
      "/var/tmp/braket/ipykernel_219268/2846668155.py:480: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(Y, dtype=torch.long).to(device)  # for CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "# from quixer.setup_training import get_train_evaluate\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Default hyperparameters for each of the models #\n",
    "##################################################\n",
    "\n",
    "quixer_hparams = {\n",
    "    \"qubits\": 6,\n",
    "    \"layers\": 3,\n",
    "    \"ansatz_layers\": 4,\n",
    "    \"window\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"restart_epochs\": 30000,\n",
    "    \"dropout\": 0.10,\n",
    "    \"lr\": 0.002,\n",
    "    \"lr_sched\": \"cos\",\n",
    "    \"wd\": 0.0001,\n",
    "    \"eps\": 1e-10,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"model\": \"Quixer\",\n",
    "    \"print_iter\": 50,\n",
    "}\n",
    "\n",
    "\n",
    "lstm_hparams = {\n",
    "    \"layers\": 2,\n",
    "    \"window\": 32,\n",
    "    \"residuals\": False,\n",
    "    \"epochs\": 30,\n",
    "    \"restart_epochs\": 30000,\n",
    "    \"dropout\": 0.30,\n",
    "    \"lr\": 0.002,\n",
    "    \"lr_sched\": \"cos\",\n",
    "    \"wd\": 0.0001,\n",
    "    \"eps\": 1e-10,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"print_iter\": 50,\n",
    "}\n",
    "\n",
    "\n",
    "fnet_hparams = {\n",
    "    \"layers\": 2,\n",
    "    \"window\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"restart_epochs\": 30000,\n",
    "    \"dropout\": 0.10,\n",
    "    \"lr\": 0.002,\n",
    "    \"lr_sched\": \"cos\",\n",
    "    \"wd\": 0.0001,\n",
    "    \"eps\": 1e-10,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"model\": \"FNet\",\n",
    "    \"print_iter\": 50,\n",
    "}\n",
    "\n",
    "\n",
    "transformer_hparams = {\n",
    "    \"layers\": 1,\n",
    "    \"heads\": 1,\n",
    "    \"window\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"restart_epochs\": 30000,\n",
    "    \"dropout\": 0.10,\n",
    "    \"lr\": 0.001,\n",
    "    \"lr_sched\": \"cos\",\n",
    "    \"wd\": 0.0001,\n",
    "    \"eps\": 1e-10,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"model\": \"Transformer\",\n",
    "    \"print_iter\": 50,\n",
    "}\n",
    "\n",
    "##################################################\n",
    "\n",
    "\n",
    "# Embedding dimensions\n",
    "classical_embedding_dimensions = [96, 128]\n",
    "quantum_embedding_dimensions = [512]\n",
    "\n",
    "# Dictionary defining available models along with associated hyperparameters\n",
    "model_map = {\n",
    "    \"Quixer\": (quixer_hparams, quantum_embedding_dimensions),\n",
    "    \"Transformer\": (transformer_hparams, classical_embedding_dimensions),\n",
    "    \"LSTM\": (lstm_hparams, classical_embedding_dimensions),\n",
    "    \"FNet\": (fnet_hparams, classical_embedding_dimensions),\n",
    "}\n",
    "available_models = list(model_map.keys())\n",
    "\n",
    "# Parse command line arguments\n",
    "args = argparse.ArgumentParser(\n",
    "    prog=\"Quixer\", description=\"Runs the Quixer model and/or classical baselines\"\n",
    ")\n",
    "args.add_argument(\n",
    "    \"-m\",\n",
    "    \"--model\",\n",
    "    default=\"Quixer\",\n",
    "    choices=available_models,\n",
    "    nargs=\"*\",\n",
    "    help=\"Model(s) to run.\",\n",
    ")\n",
    "args.add_argument(\"-d\", \"--device\", default=\"cpu\", help=\"Device to run training on.\")\n",
    "# parsed = args.parse_args()\n",
    "parsed = args.parse_args([])\n",
    "\n",
    "device_name = parsed.device\n",
    "models_to_run = parsed.model if type(parsed.model) is list else [parsed.model]\n",
    "\n",
    "# Make algorithms deterministic for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "train_evaluate = get_train_evaluate(device)\n",
    "\n",
    "\n",
    "# for model_name in models_to_run:\n",
    "#     hyperparameters, embedding_dimensions = model_map[model_name]\n",
    "#     for embedding_dimension in embedding_dimensions:\n",
    "#         for seed in torch.randint(high=1000000, size=(10,)).tolist():\n",
    "#             hyperparameters[\"model\"] = model_name\n",
    "#             hyperparameters[\"dimension\"] = embedding_dimension\n",
    "#             hyperparameters[\"seed\"] = seed\n",
    "\n",
    "#             train_evaluate(hyperparameters)\n",
    "\n",
    "for model_name in models_to_run:\n",
    "    hyperparameters, embedding_dimensions = model_map[model_name]\n",
    "    for embedding_dimension in embedding_dimensions:\n",
    "        # DON'T SHADOW seed() — rename the loop variable\n",
    "        for run_seed in torch.randint(high=1000000, size=(10,)).tolist():\n",
    "            hyperparameters[\"model\"] = model_name\n",
    "            hyperparameters[\"dimension\"] = embedding_dimension\n",
    "            hyperparameters[\"seed\"] = run_seed\n",
    "\n",
    "            train_evaluate(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438c545-4e9e-4125-9fe9-b8f4ad270bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch Env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
